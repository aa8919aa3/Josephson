#!/usr/bin/env python3
"""
é€²éšæ©Ÿå™¨å­¸ç¿’å„ªåŒ–ç³»çµ±
ä½¿ç”¨æ·±åº¦å­¸ç¿’å’Œç‰¹å¾µå·¥ç¨‹ä¾†æ”¹é€²ç´„ç‘Ÿå¤«æ£®çµæ¨¡æ“¬åƒæ•¸
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path
from sklearn.ensemble import RandomForestRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import r2_score, mean_squared_error
from scipy.optimize import differential_evolution
from scipy.stats import pearsonr
import json
import warnings
warnings.filterwarnings('ignore')

# Configure matplotlib with English fonts only
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False
plt.style.use('seaborn-v0_8')

class AdvancedMLOptimizer:
    """
    é€²éšæ©Ÿå™¨å­¸ç¿’å„ªåŒ–å™¨ï¼Œç”¨æ–¼ç´„ç‘Ÿå¤«æ£®çµåƒæ•¸å„ªåŒ–
    """
    
    def __init__(self):
        self.feature_scaler = StandardScaler()
        self.target_scaler = MinMaxScaler()
        self.models = {}
        self.feature_importance = {}
        
    def extract_advanced_features(self, y_field, Ic):
        """
        æå–é€²éšç‰¹å¾µç”¨æ–¼æ©Ÿå™¨å­¸ç¿’
        """
        features = {}
        
        # åŸºæœ¬çµ±è¨ˆç‰¹å¾µ
        features['mean'] = np.mean(Ic)
        features['std'] = np.std(Ic)
        features['cv'] = features['std'] / features['mean']
        features['skewness'] = pd.Series(Ic).skew()
        features['kurtosis'] = pd.Series(Ic).kurtosis()
        
        # ç£å ´ç›¸é—œç‰¹å¾µ
        features['field_range'] = np.ptp(y_field)
        features['field_mean'] = np.mean(y_field)
        features['field_std'] = np.std(y_field)
        
        # é »åŸŸç‰¹å¾µ
        fft = np.fft.fft(Ic)
        freq = np.fft.fftfreq(len(Ic))
        power_spectrum = np.abs(fft)**2
        
        # ä¸»è¦é »ç‡
        dominant_freq_idx = np.argmax(power_spectrum[1:len(power_spectrum)//2]) + 1
        features['dominant_frequency'] = freq[dominant_freq_idx]
        features['dominant_power'] = power_spectrum[dominant_freq_idx]
        
        # é€±æœŸæ€§ç‰¹å¾µ
        autocorr = np.correlate(Ic - np.mean(Ic), Ic - np.mean(Ic), mode='full')
        autocorr = autocorr[autocorr.size // 2:]
        autocorr = autocorr / autocorr[0]
        
        # æ‰¾åˆ°ç¬¬ä¸€å€‹å±€éƒ¨æœ€å¤§å€¼ä½œç‚ºé€±æœŸæŒ‡æ¨™
        local_maxima = []
        for i in range(1, min(len(autocorr)-1, 50)):
            if autocorr[i] > autocorr[i-1] and autocorr[i] > autocorr[i+1]:
                local_maxima.append((i, autocorr[i]))
        
        if local_maxima:
            features['primary_period'] = local_maxima[0][0]
            features['primary_period_strength'] = local_maxima[0][1]
        else:
            features['primary_period'] = 0
            features['primary_period_strength'] = 0
        
        # æ¢¯åº¦ç‰¹å¾µ
        gradient = np.gradient(Ic)
        features['gradient_mean'] = np.mean(gradient)
        features['gradient_std'] = np.std(gradient)
        features['gradient_max'] = np.max(np.abs(gradient))
        
        # å±€éƒ¨è®Šç•°ç‰¹å¾µ
        local_variations = []
        window_size = min(10, len(Ic) // 10)
        for i in range(0, len(Ic) - window_size, window_size):
            window_data = Ic[i:i+window_size]
            local_variations.append(np.std(window_data))
        
        features['local_variation_mean'] = np.mean(local_variations)
        features['local_variation_std'] = np.std(local_variations)
        
        return features
    
    def create_feature_dataset(self, exp_data_dir, sim_data_dir):
        """
        å‰µå»ºç‰¹å¾µæ•¸æ“šé›†
        """
        feature_data = []
        target_correlations = []
        
        print("ğŸ” æå–ç‰¹å¾µæ•¸æ“š...")
        
        for exp_file in exp_data_dir.glob("*.csv"):
            try:
                # è¼‰å…¥å¯¦é©—æ•¸æ“š
                exp_df = pd.read_csv(exp_file)
                exp_y = exp_df['y_field'].values
                exp_Ic = exp_df['Ic'].values
                
                # è¼‰å…¥å°æ‡‰çš„æ¨¡æ“¬æ•¸æ“š
                sim_file = sim_data_dir / f"improved_sim_{exp_file.name}"
                if sim_file.exists():
                    sim_df = pd.read_csv(sim_file)
                    sim_Ic = sim_df['Ic'].values
                    
                    # è¨ˆç®—ç›¸é—œä¿‚æ•¸ä½œç‚ºç›®æ¨™
                    correlation, _ = pearsonr(exp_Ic, sim_Ic)
                    
                    # æå–ç‰¹å¾µ
                    features = self.extract_advanced_features(exp_y, exp_Ic)
                    
                    feature_data.append(features)
                    target_correlations.append(correlation)
                    
                    print(f"âœ… è™•ç†å®Œæˆ: {exp_file.name}, ç›¸é—œä¿‚æ•¸: {correlation:.4f}")
                    
            except Exception as e:
                print(f"âŒ è™•ç† {exp_file.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        
        return pd.DataFrame(feature_data), np.array(target_correlations)
    
    def train_ml_models(self, features_df, targets):
        """
        è¨“ç·´å¤šå€‹æ©Ÿå™¨å­¸ç¿’æ¨¡å‹
        """
        print("\nğŸ¤– è¨“ç·´æ©Ÿå™¨å­¸ç¿’æ¨¡å‹...")
        
        # æº–å‚™æ•¸æ“š
        X = features_df.values
        y = targets
        
        # æ¨™æº–åŒ–ç‰¹å¾µ
        X_scaled = self.feature_scaler.fit_transform(X)
        
        # åˆ†å‰²æ•¸æ“šï¼ˆç”±æ–¼æ•¸æ“šè¼ƒå°‘ï¼Œä½¿ç”¨è¼ƒå°çš„æ¸¬è©¦é›†ï¼‰
        if len(X) > 6:
            X_train, X_test, y_train, y_test = train_test_split(
                X_scaled, y, test_size=0.3, random_state=42
            )
        else:
            X_train, X_test, y_train, y_test = X_scaled, X_scaled, y, y
        
        # è¨“ç·´æ¨¡å‹
        models_config = {
            'random_forest': {
                'model': RandomForestRegressor(n_estimators=100, random_state=42),
                'params': {}
            },
            'neural_network': {
                'model': MLPRegressor(hidden_layer_sizes=(50, 25), max_iter=1000, random_state=42),
                'params': {}
            }
        }
        
        results = {}
        
        for name, config in models_config.items():
            print(f"   è¨“ç·´ {name}...")
            
            model = config['model']
            model.fit(X_train, y_train)
            
            # è©•ä¼°
            train_pred = model.predict(X_train)
            test_pred = model.predict(X_test)
            
            train_r2 = r2_score(y_train, train_pred)
            test_r2 = r2_score(y_test, test_pred)
            train_mse = mean_squared_error(y_train, train_pred)
            test_mse = mean_squared_error(y_test, test_pred)
            
            results[name] = {
                'model': model,
                'train_r2': train_r2,
                'test_r2': test_r2,
                'train_mse': train_mse,
                'test_mse': test_mse
            }
            
            # ç‰¹å¾µé‡è¦æ€§ï¼ˆåƒ…å°éš¨æ©Ÿæ£®æ—ï¼‰
            if hasattr(model, 'feature_importances_'):
                importance = dict(zip(features_df.columns, model.feature_importances_))
                self.feature_importance[name] = importance
            
            print(f"      è¨“ç·´ RÂ²: {train_r2:.4f}, æ¸¬è©¦ RÂ²: {test_r2:.4f}")
        
        self.models = results
        return results
    
    def optimize_parameters_with_ml(self, exp_file, feature_target=0.5):
        """
        ä½¿ç”¨æ©Ÿå™¨å­¸ç¿’æŒ‡å°åƒæ•¸å„ªåŒ–
        """
        print(f"\nğŸ¯ ç‚º {exp_file.name} å„ªåŒ–åƒæ•¸...")
        
        # è¼‰å…¥å¯¦é©—æ•¸æ“š
        exp_df = pd.read_csv(exp_file)
        exp_y = exp_df['y_field'].values
        exp_Ic = exp_df['Ic'].values
        
        # æå–ç‰¹å¾µ
        features = self.extract_advanced_features(exp_y, exp_Ic)
        feature_vector = np.array([features[col] for col in features.keys()]).reshape(1, -1)
        feature_vector_scaled = self.feature_scaler.transform(feature_vector)
        
        # ä½¿ç”¨æœ€ä½³æ¨¡å‹é æ¸¬ç›®æ¨™ç›¸é—œä¿‚æ•¸
        best_model_name = max(self.models.keys(), 
                             key=lambda x: self.models[x]['test_r2'])
        best_model = self.models[best_model_name]['model']
        
        predicted_correlation = best_model.predict(feature_vector_scaled)[0]
        print(f"   é æ¸¬ç›¸é—œä¿‚æ•¸: {predicted_correlation:.4f}")
        
        # å¦‚æœé æ¸¬ç›¸é—œä¿‚æ•¸ä½æ–¼ç›®æ¨™ï¼Œä½¿ç”¨å·®åˆ†é€²åŒ–ç®—æ³•å„ªåŒ–
        if predicted_correlation < feature_target:
            print("   é–‹å§‹åƒæ•¸å„ªåŒ–...")
            
            def objective_function(params):
                """å„ªåŒ–ç›®æ¨™å‡½æ•¸"""
                Ic_base, field_scale, phase_offset, asymmetry, noise_level = params
                
                try:
                    # ç”Ÿæˆæ¨¡æ“¬æ•¸æ“š
                    flux_quantum = 2.067e-15
                    phi_ext = exp_y * field_scale
                    normalized_flux = np.pi * phi_ext / flux_quantum
                    
                    with np.errstate(divide='ignore', invalid='ignore'):
                        sinc_term = np.sin(normalized_flux + phase_offset) / (normalized_flux + 1e-10)
                        sinc_term = np.where(np.abs(normalized_flux) < 1e-10, 1.0, sinc_term)
                    
                    Ic_pattern = np.abs(sinc_term) * (1 + asymmetry * np.cos(2 * normalized_flux))
                    Ic_sim = Ic_base * Ic_pattern
                    
                    # æ·»åŠ å™ªè²
                    noise = np.random.normal(0, noise_level, len(Ic_sim))
                    Ic_sim += noise
                    
                    # è¨ˆç®—ç›¸é—œä¿‚æ•¸
                    correlation, _ = pearsonr(exp_Ic, Ic_sim)
                    
                    return -correlation if not np.isnan(correlation) else -1e-6
                    
                except:
                    return 1.0  # æ‡²ç½°ç„¡æ•ˆåƒæ•¸
            
            # å®šç¾©åƒæ•¸é‚Šç•Œ
            bounds = [
                (np.mean(exp_Ic) * 0.1, np.mean(exp_Ic) * 3.0),  # Ic_base
                (1e-10, 1e-5),                                    # field_scale
                (0, 2*np.pi),                                     # phase_offset
                (-0.5, 0.5),                                      # asymmetry
                (np.std(exp_Ic) * 0.01, np.std(exp_Ic) * 0.5)   # noise_level
            ]
            
            # åŸ·è¡Œå„ªåŒ–
            result = differential_evolution(
                objective_function,
                bounds,
                maxiter=100,
                popsize=15,
                seed=42
            )
            
            optimized_params = {
                'Ic_base': result.x[0],
                'field_scale': result.x[1],
                'phase_offset': result.x[2],
                'asymmetry': result.x[3],
                'noise_level': result.x[4],
                'predicted_correlation': -result.fun
            }
            
            print(f"   å„ªåŒ–å¾Œé æœŸç›¸é—œä¿‚æ•¸: {-result.fun:.4f}")
            
        else:
            print("   é æ¸¬ç›¸é—œä¿‚æ•¸å·²è¶³å¤ ï¼Œè·³éå„ªåŒ–")
            optimized_params = None
        
        return optimized_params
    
    def generate_enhanced_report(self, results_dir):
        """
        ç”Ÿæˆå¢å¼·åˆ†æå ±å‘Š
        """
        report_path = results_dir / "ml_optimization_report.md"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# æ©Ÿå™¨å­¸ç¿’å„ªåŒ–åˆ†æå ±å‘Š\n\n")
            f.write(f"ç”Ÿæˆæ™‚é–“: 2025å¹´6æœˆ3æ—¥\n\n")
            
            f.write("## æ¨¡å‹æ€§èƒ½\n\n")
            for name, results in self.models.items():
                f.write(f"### {name.title()}\n")
                f.write(f"- è¨“ç·´ RÂ²: {results['train_r2']:.4f}\n")
                f.write(f"- æ¸¬è©¦ RÂ²: {results['test_r2']:.4f}\n")
                f.write(f"- è¨“ç·´ MSE: {results['train_mse']:.6f}\n")
                f.write(f"- æ¸¬è©¦ MSE: {results['test_mse']:.6f}\n\n")
            
            f.write("## ç‰¹å¾µé‡è¦æ€§\n\n")
            for model_name, importance in self.feature_importance.items():
                f.write(f"### {model_name.title()}\n")
                sorted_features = sorted(importance.items(), key=lambda x: x[1], reverse=True)
                for feature, score in sorted_features[:10]:
                    f.write(f"- {feature}: {score:.4f}\n")
                f.write("\n")
            
            f.write("## å»ºè­°\n\n")
            f.write("1. **ç‰¹å¾µå·¥ç¨‹**: åŸºæ–¼é‡è¦æ€§åˆ†æï¼Œé‡é»é—œæ³¨é«˜æ¬Šé‡ç‰¹å¾µ\n")
            f.write("2. **æ¨¡å‹é¸æ“‡**: é¸æ“‡æ¸¬è©¦é›†è¡¨ç¾æœ€ä½³çš„æ¨¡å‹é€²è¡Œé æ¸¬\n")
            f.write("3. **åƒæ•¸å„ªåŒ–**: å°ä½ç›¸é—œä¿‚æ•¸æ¨£å“é€²è¡Œé‡é»å„ªåŒ–\n")
            f.write("4. **æ•¸æ“šæ“´å……**: è€ƒæ…®æ”¶é›†æ›´å¤šå¯¦é©—æ•¸æ“šä»¥æ”¹å–„æ¨¡å‹æ€§èƒ½\n")
        
        print(f"âœ… å ±å‘Šå·²ä¿å­˜è‡³: {report_path}")

def main():
    """
    ä¸»åŸ·è¡Œå‡½æ•¸
    """
    print("=== é€²éšæ©Ÿå™¨å­¸ç¿’å„ªåŒ–ç³»çµ± ===\n")
    
    # è¨­å®šè·¯å¾‘
    base_dir = Path("/Users/albert-mac/Code/GitHub/Josephson")
    exp_data_dir = base_dir / "data" / "experimental"
    sim_data_dir = base_dir / "data" / "simulated"
    results_dir = base_dir / "results"
    results_dir.mkdir(exist_ok=True)
    
    # å‰µå»ºå„ªåŒ–å™¨
    optimizer = AdvancedMLOptimizer()
    
    # å‰µå»ºç‰¹å¾µæ•¸æ“šé›†
    features_df, targets = optimizer.create_feature_dataset(exp_data_dir, sim_data_dir)
    
    print(f"\nğŸ“Š ç‰¹å¾µæ•¸æ“šé›†æ‘˜è¦:")
    print(f"   æ¨£æœ¬æ•¸é‡: {len(features_df)}")
    print(f"   ç‰¹å¾µæ•¸é‡: {len(features_df.columns)}")
    print(f"   ç›®æ¨™ç¯„åœ: {targets.min():.4f} åˆ° {targets.max():.4f}")
    
    # è¨“ç·´æ¨¡å‹
    model_results = optimizer.train_ml_models(features_df, targets)
    
    # å„ªåŒ–åƒæ•¸
    optimization_results = {}
    for exp_file in exp_data_dir.glob("*.csv"):
        if exp_file.name in ['317Ic.csv', '335Ic.csv', '337Ic.csv']:  # é¸æ“‡å¹¾å€‹æ¨£æœ¬é€²è¡Œå„ªåŒ–
            opt_params = optimizer.optimize_parameters_with_ml(exp_file, feature_target=0.3)
            if opt_params:
                optimization_results[exp_file.name] = opt_params
    
    # ä¿å­˜å„ªåŒ–çµæœ
    if optimization_results:
        opt_file = results_dir / "ml_optimized_parameters.json"
        with open(opt_file, 'w', encoding='utf-8') as f:
            json.dump(optimization_results, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ å„ªåŒ–åƒæ•¸å·²ä¿å­˜è‡³: {opt_file}")
    
    # ç”Ÿæˆå ±å‘Š
    optimizer.generate_enhanced_report(results_dir)
    
    print("\nâœ… æ©Ÿå™¨å­¸ç¿’å„ªåŒ–å®Œæˆï¼")

if __name__ == "__main__":
    main()
