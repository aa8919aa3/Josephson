"""
çµ±è¨ˆè©•ä¼°å·¥å…·

æä¾›å®Œæ•´çš„æ¨¡å‹çµ±è¨ˆè©•ä¼°åŠŸèƒ½ï¼ŒåŒ…æ‹¬ RÂ²ã€RMSEã€æ¨¡å‹æ¯”è¼ƒç­‰ã€‚
"""

import numpy as np
import pandas as pd
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.io as pio
from scipy import stats

# è¨­å®š Plotly ç‚ºéäº’å‹•æ¨¡å¼ï¼Œé¿å…çµ‚ç«¯è¼¸å‡ºå•é¡Œ
pio.renderers.default = "json"

class ModelStatistics:
    """
    æ¨¡å‹çµ±è¨ˆè©•ä¼°é¡åˆ¥
    
    æä¾›å®Œæ•´çš„å›æ­¸åˆ†æçµ±è¨ˆæŒ‡æ¨™è¨ˆç®—å’Œè¨ºæ–·ã€‚
    """
    
    def __init__(self, y_true, y_pred, n_params=None, model_name="Model"):
        """
        åˆå§‹åŒ–çµ±è¨ˆè©•ä¼°
        
        Parameters:
        -----------
        y_true : array-like
            çœŸå¯¦è§€æ¸¬å€¼
        y_pred : array-like
            æ¨¡å‹é æ¸¬å€¼
        n_params : int, optional
            æ¨¡å‹åƒæ•¸æ•¸é‡
        model_name : str
            æ¨¡å‹åç¨±
        """
        self.y_true = np.array(y_true)
        self.y_pred = np.array(y_pred)
        self.model_name = model_name
        
        # ç§»é™¤ NaN å€¼
        self.valid_mask = ~(np.isnan(self.y_true) | np.isnan(self.y_pred))
        self.y_true_clean = self.y_true[self.valid_mask]
        self.y_pred_clean = self.y_pred[self.valid_mask]
        
        self.n = len(self.y_true_clean)
        self.n_params = n_params if n_params is not None else 1
        
        # è¨ˆç®—æ‰€æœ‰çµ±è¨ˆæŒ‡æ¨™
        self._calculate_all_statistics()
    
    def _calculate_all_statistics(self):
        """è¨ˆç®—æ‰€æœ‰çµ±è¨ˆæŒ‡æ¨™"""
        if self.n == 0:
            self._set_invalid_stats()
            return
        
        # åŸºæœ¬çµ±è¨ˆé‡
        self.residuals = self.y_true_clean - self.y_pred_clean
        self.y_mean = np.mean(self.y_true_clean)
        
        # å¹³æ–¹å’Œè¨ˆç®—
        self.sse = np.sum(self.residuals**2)  # Sum of Squared Errors
        self.sst = np.sum((self.y_true_clean - self.y_mean)**2)  # Total Sum of Squares
        self.ssr = np.sum((self.y_pred_clean - self.y_mean)**2)  # Sum of Squares Regression
        
        # R-squared ç›¸é—œæŒ‡æ¨™
        self.r_squared = self._calculate_r_squared()
        self.adjusted_r_squared = self._calculate_adjusted_r_squared()
        
        # èª¤å·®æŒ‡æ¨™
        self.rmse = np.sqrt(self.sse / self.n)  # Root Mean Squared Error
        self.mae = np.mean(np.abs(self.residuals))  # Mean Absolute Error
        self.mse = self.sse / self.n  # Mean Squared Error
        self.mape = self._calculate_mape()  # Mean Absolute Percentage Error
        
        # ä¿¡æ¯æº–å‰‡
        self.aic = self._calculate_aic()
        self.bic = self._calculate_bic()
        
        # å…¶ä»–æŒ‡æ¨™
        self.correlation = self._calculate_correlation()
        self.explained_variance = self._calculate_explained_variance()
    
    def _calculate_r_squared(self):
        """è¨ˆç®—æ±ºå®šä¿‚æ•¸ (R-squared)"""
        if self.sst == 0:
            return 1.0 if self.sse < 1e-10 else 0.0
        return 1 - (self.sse / self.sst)
    
    def _calculate_adjusted_r_squared(self):
        """è¨ˆç®—èª¿æ•´å¾ŒRå¹³æ–¹ (Adjusted R-squared)"""
        if self.n <= self.n_params:
            return np.nan
        
        if self.sst == 0:
            return 1.0 if self.sse < 1e-10 else 0.0
        
        return 1 - ((self.sse / (self.n - self.n_params - 1)) / (self.sst / (self.n - 1)))
    
    def _calculate_mape(self):
        """è¨ˆç®—å¹³å‡çµ•å°ç™¾åˆ†æ¯”èª¤å·®"""
        non_zero_mask = np.abs(self.y_true_clean) > 1e-10
        if not np.any(non_zero_mask):
            return np.nan
        
        mape_values = np.abs(self.residuals[non_zero_mask] / self.y_true_clean[non_zero_mask]) * 100
        return np.mean(mape_values)
    
    def _calculate_aic(self):
        """è¨ˆç®— Akaike Information Criterion"""
        if self.n <= 0 or self.sse <= 0:
            return np.nan
        return self.n * np.log(self.sse / self.n) + 2 * self.n_params
    
    def _calculate_bic(self):
        """è¨ˆç®— Bayesian Information Criterion"""
        if self.n <= 0 or self.sse <= 0:
            return np.nan
        return self.n * np.log(self.sse / self.n) + self.n_params * np.log(self.n)
    
    def _calculate_correlation(self):
        """è¨ˆç®—ç›¸é—œä¿‚æ•¸"""
        if len(self.y_true_clean) < 2:
            return np.nan
        return np.corrcoef(self.y_true_clean, self.y_pred_clean)[0, 1]
    
    def _calculate_explained_variance(self):
        """è¨ˆç®—è§£é‡‹è®Šç•°æ•¸"""
        var_y = np.var(self.y_true_clean)
        if var_y == 0:
            return 1.0 if np.var(self.residuals) < 1e-10 else 0.0
        return 1 - np.var(self.residuals) / var_y
    
    def get_summary_dict(self):
        """è¿”å›çµ±è¨ˆæ‘˜è¦å­—å…¸"""
        return {
            'model_name': self.model_name,
            'n_observations': self.n,
            'n_parameters': self.n_params,
            'r_squared': self.r_squared,
            'adjusted_r_squared': self.adjusted_r_squared,
            'rmse': self.rmse,
            'mae': self.mae,
            'mape': self.mape,
            'mse': self.mse,
            'sse': self.sse,
            'sst': self.sst,
            'ssr': self.ssr,
            'aic': self.aic,
            'bic': self.bic,
            'correlation': self.correlation,
            'explained_variance': self.explained_variance
        }
    
    def print_summary(self):
        """åˆ—å°è©³ç´°çµ±è¨ˆæ‘˜è¦"""
        print(f"\n{'='*60}")
        print(f"ğŸ“Š {self.model_name} çµ±è¨ˆè©•ä¼°æ‘˜è¦")
        print(f"{'='*60}")
        
        print(f"ğŸ”¢ åŸºæœ¬ä¿¡æ¯:")
        print(f"   è§€æ¸¬å€¼æ•¸é‡: {self.n:,}")
        print(f"   æ¨¡å‹åƒæ•¸æ•¸é‡: {self.n_params}")
        
        print(f"\nğŸ“ˆ æ“¬åˆå“è³ªæŒ‡æ¨™:")
        print(f"   R-squared (æ±ºå®šä¿‚æ•¸): {self.r_squared:.6f}")
        print(f"   Adjusted RÂ² (èª¿æ•´å¾ŒRÂ²): {self.adjusted_r_squared:.6f}")
        print(f"   ç›¸é—œä¿‚æ•¸: {self.correlation:.6f}")
        
        print(f"\nğŸ“ èª¤å·®æŒ‡æ¨™:")
        print(f"   RMSE (å‡æ–¹æ ¹èª¤å·®): {self.rmse:.6e}")
        print(f"   MAE (å¹³å‡çµ•å°èª¤å·®): {self.mae:.6e}")
        print(f"   MAPE (å¹³å‡çµ•å°ç™¾åˆ†æ¯”èª¤å·®): {self.mape:.2f}%")
        
        print(f"\nğŸ“Š å¹³æ–¹å’Œåˆ†è§£:")
        print(f"   SSE (èª¤å·®å¹³æ–¹å’Œ): {self.sse:.6e}")
        print(f"   SSR (å›æ­¸å¹³æ–¹å’Œ): {self.ssr:.6e}")
        print(f"   SST (ç¸½å¹³æ–¹å’Œ): {self.sst:.6e}")
        
        print(f"\nğŸ” æ¨¡å‹é¸æ“‡æº–å‰‡:")
        print(f"   AIC: {self.aic:.2f}")
        print(f"   BIC: {self.bic:.2f}")

def compare_multiple_models(*stats_objects, plot_comparison=True):
    """
    æ¯”è¼ƒå¤šå€‹æ¨¡å‹çš„çµ±è¨ˆæŒ‡æ¨™
    
    Parameters:
    -----------
    *stats_objects : ModelStatistics objects
        è¦æ¯”è¼ƒçš„çµ±è¨ˆç‰©ä»¶
    plot_comparison : bool
        æ˜¯å¦ç¹ªè£½æ¯”è¼ƒåœ–è¡¨
    
    Returns:
    --------
    pd.DataFrame : æ¯”è¼ƒçµæœè¡¨æ ¼
    """
    if len(stats_objects) == 0:
        print("âŒ æ²’æœ‰æä¾›ä»»ä½•æ¨¡å‹é€²è¡Œæ¯”è¼ƒ")
        return None
    
    # æ”¶é›†æ‰€æœ‰çµ±è¨ˆæŒ‡æ¨™
    comparison_data = []
    for stats_obj in stats_objects:
        summary = stats_obj.get_summary_dict()
        comparison_data.append(summary)
    
    # å»ºç«‹æ¯”è¼ƒè¡¨æ ¼
    comparison_df = pd.DataFrame(comparison_data)
    comparison_df = comparison_df.set_index('model_name')
    
    # åˆ—å°æ¯”è¼ƒçµæœ
    print(f"\n{'='*80}")
    print(f"ğŸ† æ¨¡å‹æ¯”è¼ƒçµæœ")
    print(f"{'='*80}")
    
    print("\nğŸ“Š ä¸»è¦æ“¬åˆæŒ‡æ¨™:")
    print(comparison_df[['r_squared', 'adjusted_r_squared', 'correlation']].round(6))
    
    print("\nğŸ“ èª¤å·®æŒ‡æ¨™:")
    print(comparison_df[['rmse', 'mae', 'mape']].round(6))
    
    print("\nğŸ” æ¨¡å‹é¸æ“‡æº–å‰‡ (è¶Šå°è¶Šå¥½):")
    print(comparison_df[['aic', 'bic']].round(2))
    
    # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
    best_models = {}
    metrics = ['r_squared', 'adjusted_r_squared', 'correlation']
    for metric in metrics:
        if metric in comparison_df.columns:
            best_idx = comparison_df[metric].idxmax()
            best_models[metric] = (best_idx, comparison_df.loc[best_idx, metric])
    
    error_metrics = ['rmse', 'mae', 'aic', 'bic']
    for metric in error_metrics:
        if metric in comparison_df.columns:
            best_idx = comparison_df[metric].idxmin()
            best_models[metric] = (best_idx, comparison_df.loc[best_idx, metric])
    
    print(f"\nğŸ¥‡ å„æŒ‡æ¨™æœ€ä½³æ¨¡å‹:")
    for metric, (model_name, value) in best_models.items():
        print(f"   {metric.upper()}: {model_name} ({value:.6f})")
    
    return comparison_df

def plot_comprehensive_model_diagnostics(stats_obj, phi_ext=None, title_prefix=""):
    """
    ç¹ªè£½å®Œæ•´çš„æ¨¡å‹è¨ºæ–·åœ–
    
    Parameters:
    -----------
    stats_obj : ModelStatistics
        çµ±è¨ˆç‰©ä»¶
    phi_ext : array-like, optional
        å¤–éƒ¨ç£é€šæ•¸æ“šï¼ˆç”¨æ–¼ x è»¸ï¼‰
    title_prefix : str
        åœ–è¡¨æ¨™é¡Œå‰ç¶´
    """
    
    # å»ºç«‹å­åœ–
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=(
            'çœŸå¯¦å€¼ vs é æ¸¬å€¼', 'æ®˜å·® vs é æ¸¬å€¼',
            'æ®˜å·®ç›´æ–¹åœ–', 'Q-Q æ­£æ…‹æª¢é©—åœ–'
        )
    )
    
    # 1. çœŸå¯¦å€¼ vs é æ¸¬å€¼
    fig.add_trace(
        go.Scatter(x=stats_obj.y_true_clean, y=stats_obj.y_pred_clean,
                  mode='markers', name='æ•¸æ“šé»',
                  marker=dict(size=5, opacity=0.6, color='blue')),
        row=1, col=1
    )
    
    # æ·»åŠ  y=x åƒè€ƒç·š
    min_val = min(stats_obj.y_true_clean.min(), stats_obj.y_pred_clean.min())
    max_val = max(stats_obj.y_true_clean.max(), stats_obj.y_pred_clean.max())
    fig.add_trace(
        go.Scatter(x=[min_val, max_val], y=[min_val, max_val],
                  mode='lines', name='å®Œç¾æ“¬åˆç·š',
                  line=dict(color='red', dash='dash')),
        row=1, col=1
    )
    
    # 2. æ®˜å·® vs é æ¸¬å€¼
    fig.add_trace(
        go.Scatter(x=stats_obj.y_pred_clean, y=stats_obj.residuals,
                  mode='markers', name='æ®˜å·®',
                  marker=dict(size=5, opacity=0.6, color='green')),
        row=1, col=2
    )
    fig.add_hline(y=0, line_dash="dash", line_color="red", row=1, col=2)
    
    # 3. æ®˜å·®ç›´æ–¹åœ–
    fig.add_trace(
        go.Histogram(x=stats_obj.residuals, name='æ®˜å·®åˆ†å¸ƒ',
                    nbinsx=30, opacity=0.7),
        row=2, col=1
    )
    
    # 4. Q-Q åœ–
    if len(stats_obj.residuals) > 2:
        sorted_residuals = np.sort(stats_obj.residuals)
        n = len(sorted_residuals)
        theoretical_quantiles = stats.norm.ppf(np.arange(1, n+1) / (n+1))
        
        fig.add_trace(
            go.Scatter(x=theoretical_quantiles, y=sorted_residuals,
                      mode='markers', name='Q-Q é»',
                      marker=dict(size=4, opacity=0.6, color='purple')),
            row=2, col=2
        )
        
        # Q-Q åƒè€ƒç·š
        qq_slope, qq_intercept = np.polyfit(theoretical_quantiles, sorted_residuals, 1)
        qq_line_y = qq_slope * theoretical_quantiles + qq_intercept
        fig.add_trace(
            go.Scatter(x=theoretical_quantiles, y=qq_line_y,
                      mode='lines', name='Q-Q åƒè€ƒç·š',
                      line=dict(color='red', dash='dash')),
            row=2, col=2
        )
    
    # æ›´æ–°ä½ˆå±€
    fig.update_layout(
        title_text=f"{title_prefix} æ¨¡å‹è¨ºæ–·åˆ†æ - {stats_obj.model_name}",
        height=600,
        showlegend=True
    )
    
    # æ›´æ–°è»¸æ¨™é¡Œ
    fig.update_xaxes(title_text="çœŸå¯¦å€¼", row=1, col=1)
    fig.update_yaxes(title_text="é æ¸¬å€¼", row=1, col=1)
    fig.update_xaxes(title_text="é æ¸¬å€¼", row=1, col=2)
    fig.update_yaxes(title_text="æ®˜å·®", row=1, col=2)
    fig.update_xaxes(title_text="æ®˜å·®", row=2, col=1)
    fig.update_yaxes(title_text="é »ç‡", row=2, col=1)
    fig.update_xaxes(title_text="ç†è«–åˆ†ä½æ•¸", row=2, col=2)
    fig.update_yaxes(title_text="æ¨£æœ¬åˆ†ä½æ•¸", row=2, col=2)
    
    fig.show()